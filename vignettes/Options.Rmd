<!--
%\VignetteEngine{knitr}
%\VignetteIndexEntry{Using Optional Arguments}
-->

# Using Optional Arguments in `margins` #

**margins** is intended as a port of (some of) the features of Stata's `margins` command, which includes numerous options for calculating marginal effects at the mean values of a dataset (i.e., the marginal effects at the mean), an average of the marginal effects at each value of a dataset (i.e., the average marginal effect), marginal effects at representative values, and any of those operations on various subsets of a dataset. In particular, Stata provides the following options:

 - `atmeans`: calculate marginal effects at the mean (MEMs) of a dataset rather than the default behavior of calculating average marginal effects (AMEs)
 - `at`: calculate marginal effects at (potentially representative) specified values (i.e., replacing observed values with specified replacement values before calculating marginal effects)
 - `over`: calculate marginal effects (including MEMs and/or AMEs at observed or specified values) on subsets of the original data (e.g., the marginal effect of a treatment separately for men and women)
 
Stata's `atmeans` argument is translated into `margins` as a simple logical argument. The default (`atmeans = FALSE`) produces AMEs; `atmeans = TRUE` produces MEMs.

The `at` argument has also been translated into `margins`. It can be used by specifying a list of variable names and specified values for those variables at which to calculate marginal effects. When using `at`, `margins` constructs modified datasets containing the specified values and calculates marginal effects on each modified dataset.

At present, `margins` does not implement the `over` option. The reason for this is simple: R already makes data subsetting operations quite simple using simple `[` extraction. If, for example, one wanted to calculate marginal effects on subsets of a data.frame, those subsets can be passed directly to `margins` via the `newdata` argument (as in a call to `predict` from the **stats** package).

The rest of this vignette shows how to use `at`, `atmeans`, and `newdata` to obtain various kinds of marginal effects.

---

## AMEs versus MEMs ##

```{r, echo = FALSE, results = 'hide'}
options(width = 100)
```

We can start by loading the **margins** package:

```{r}
library("margins")
```

We'll use a simple example regression model based on the `mtcars` dataset:

```{r}
x <- lm(mpg ~ cyl + hp * wt, data = mtcars)
```

To obtain average marginal effects (AMEs), we simply call `margins` on the model object created by `lm`:

```{r}
margins(x)
```

The result is a list containing a single object of class `"margins"`. `"margins"` objects are printed in a tidy summary format, by default, as you can see above. To instead obtain marginal effects at the means (MEMs), we simply add `atmeans = TRUE` to the function call:

```{r}
margins(x, atmeans = TRUE)
```

Of course in an ordinary least squares regression, this option makes no difference for the resulting marginal effects because the regression coefficients are marginal effects. In a generalized linear model (e.g., logit), however, this difference would be consequential as can be seen in the trivial example below. Note that if marginal effects were calculated on the log-odds or latent scale (using option `type = "link"`), the `atmeans` option would also be inconsequential. But, examing marginal effects on the probability scale (using option `type = "response"`), there is a difference in the apparent marginal effects of the terms included in the explicit interaction: `hp` and `wt`. In a large sample, the difference between AMEs and MEMs would likely disappear.

```{r}
x <- glm(am ~ cyl + hp * wt, data = mtcars, family = binomial)
# AMEs
margins(x, type = "response")
# MEMs
margins(x, atmeans = TRUE, type = "response")
```

---

## Using the `at` Argument ##

The `at` argument allows you to calculate marginal effects at representative cases (sometimes "MERs"), which are marginal effects for particularly interesting (sets of) observations in a dataset. This differs from marginal effects on subsets of the original data (see the next section for a demonstration of that). This is helpful because it allows for calculation of marginal effects for counterfactual datasets. For example, if we wanted to know if the marginal effect of horsepower (`hp`) on fuel economy differed across different types of transmissions, we could simply use `at` to obtain separate marginal effect estimates for our data as if every car observation were a manual versus if every car observation were an automatic.

```{r, results = "hold"}
x <- lm(mpg ~ cyl + wt + hp * am, data = mtcars)
margins(x, at = list(am = 0:1))
```

Because of the `hp * am` interaction in the regression, the marginal effect of horsepower differs between the two sets of results. We can also specify more than one variable to `at`, creating a potentially long list of marginal effects results. For example, we can produce marginal effects at both levels of `am` and the values from the five-number summary (minimum, Q1, median, Q3, and maximum) of observed values of `hp`. This produces 2 * 5 = 10 sets of marginal effects estimates. We'll see the first three below:

```{r, results = "hold"}
m <- margins(x, at = list(am = 0:1, hp = fivenum(mtcars$hp)))
m[1:3] # first three summaries
```

Because this is a linear model, the marginal effects of `cyl` and `wt` do not vary across levels of `am` or `hp`. The minimum and Q1 value of `hp` are also the same, so the marginal effects of `am` are the same in the first two results tables. As you can see, however, the marginal effect of `hp` differs when `am == 0` versus `am == 1` (first and second results tables) and the marginal effect of `am` differs across levels of `hp` (e.g., between the first and third tables). As should be clear, the `at` argument is incredibly useful for getting a better grasp of the marginal effects of different covariates.

This becomes especially apparent when a model includes power-terms (or any other alternative form of a covariate). Consider, for example, the simple model of fuel economy as a function of weight, with weight included as both a first- and second-order term:

```{r, results = "hold"}
x <- lm(mpg ~ wt + I(wt^2), data = mtcars)
summary(x)
```

Looking only at the regression results table, it is actually quite difficult to understand the effect of `wt` on fuel economy because it requires performing mental multiplication and addition on all possible values of `wt`. Using the `at` option to margins, we can quickly obtain a sense of the average marginal effect of `wt` at a range of plausible values:

```{r, results = "hold"}
wt_tmp <- fivenum(mtcars$wt)
m <- margins(x, at = list(wt = wt_tmp))
t(sapply(m, summary))[,-1]
```

The marginal effects in the first column of results reveal that the average marginal effect of `wt` is large and negative except when `wt` is very large, in which case it has an effect not distinguishable from zero. We can easily plot these results to see the effect visually:

```{r}
w <- t(sapply(m, summary))[,-1]
plot(wt_tmp, w[,1], type = "l", ylim = c(-15,5), lwd = 2,
     xlab = "wt", ylab = "Average Marginal Effect of Weight")
abline(h = 0, col = 'gray')
lines(wt_tmp, w[,5], lty = 2)
lines(wt_tmp, w[,6], lty = 2)
```

---

## Marginal Effects on Subsets ##

As noted above, Stata includes an `over` option to allow calculation of marginal effects on subsets of a dataset. This is excluded from `margins` here because subsetting in R is easy and because the distinction between Stata's `at` and `over` options can be confusing. To obtain marginal effects for subsets, let's consider a regression model that includes an indicator (dummy) variable interaction. In this case, we use the `am` variable which specifies whether a car has an automatic (0) or manual (1) transmission:

```{r}
x <- lm(mpg ~ cyl + wt + hp * am, data = mtcars)
summary(x)
```

To obtain the marginal effects of `hp` in this model separately for automatic and manual transmission cars, we simply need to subset the `mtcars` dataset and pass those subsets to the `newdata` option in `margins`:

```{r}
dat <- split(mtcars, mtcars$am)

# automatic cars
margins(x, newdata = dat[[1]])

# manual cars
margins(x, newdata = dat[[2]])
```

This separation makes the marginal effects of `hp` extremely clear. The coefficient `hp` in the model is *-0.008945*. The coefficient on the `hp:am` interaction is *-0.017446*. The `margins` results for automatic cars (the first result above) shows the marginal effect of `hp` to be *-0.008945198*, which matches the coefficient exactly. The results for manual cars (the second result above) shows the marginal effect of `hp` to be *-0.02639158*, which is exactly the sum of the coefficients for `hp` and `hp:am`. As such, `margins` on these subsets helps clarify the substantive results of the regression models.
